[
  {
    "url": "https://www.wired.com/story/inside-the-biden-administrations-unpublished-report-on-ai-safety/",
    "title": "Inside the US Government's Unpublished Report on AI Safety",
    "text": "At a computer security conference in Arlington, Virginia, last October, a few dozen AI researchers took part in a first-of-its-kind exercise in \u201cred teaming,\u201d or stress-testing a cutting-edge language model and other artificial intelligence systems. Over the course of two days, the teams identified 139 novel ways to get the systems to misbehave including by generating misinformation or leaking personal data. More importantly, they showed shortcomings in a new US government standard designed to help companies test AI systems.\n\nThe National Institute of Standards and Technology (NIST) didn\u2019t publish a report detailing the exercise, which was finished toward the end of the Biden administration. The document might have helped companies assess their own AI systems, but sources familiar with the situation, who spoke on condition of anonymity, say it was one of several AI documents from NIST that were not published for fear of clashing with the incoming administration. (WIRED is publishing the report in full here).\n\n\u201cIt became very difficult, even under [president Joe] Biden, to get any papers out,\u201d says a source who was at NIST at the time. \u201cIt felt very like climate change research or cigarette research.\u201d\n\nNeither NIST nor the Commerce Department responded to a request for comment.\n\nBefore taking office, President Donald Trump signaled that he planned to reverse Biden\u2019s Executive Order on AI. Trump\u2019s administration has since steered experts away from studying issues such as algorithmic bias or fairness in AI systems. The AI Action plan released in July explicitly calls for NIST\u2019s AI Risk Management Framework to be revised \u201cto eliminate references to misinformation, Diversity, Equity, and Inclusion, and climate change.\u201d\n\nIronically, though, Trump\u2019s AI Action plan also calls for exactly the kind of exercise that the unpublished report covered. It calls for numerous agencies along with NIST to \u201ccoordinate an AI hackathon initiative to solicit the best and brightest from US academia to test AI systems for transparency, effectiveness, use control, and security vulnerabilities.\u201d\n\nThe red-teaming event was organized through NIST\u2019s Assessing Risks and Impacts of AI (ARIA) program in collaboration with Humane Intelligence, a company that specializes in testing AI systems saw teams attack tools. The event took place at the Conference on Applied Machine Learning in Information Security (CAMLIS).\n\nThe CAMLIS Red Teaming report describes the effort to probe several cutting edge AI systems including Llama, Meta\u2019s open source large language model; Anote, a platform for building and fine-tuning AI models; a system that blocks attacks on AI systems from Robust Intelligence, a company that was acquired by CISCO; and a platform for generating AI avatars from the firm Synthesia. Representatives from each of the companies also took part in the exercise.\n\nParticipants were asked to use the NIST AI 600-1 framework to assess AI tools. The framework covers risk categories including generating misinformation or cybersecurity attacks, leaking private user information or critical information about related AI systems, and the potential for users to become emotionally attached to AI tools.",
    "summary": "More importantly, they showed shortcomings in a new US government standard designed to help companies test AI systems.\nTrump\u2019s administration has since steered experts away from studying issues such as algorithmic bias or fairness in AI systems.\nThe event took place at the Conference on Applied Machine Learning in Information Security (CAMLIS).\nParticipants were asked to use the NIST AI 600-1 framework to assess AI tools.\nThe framework covers risk categories including generating misinformation or cybersecurity attacks, leaking private user information or critical information about related AI systems, and the potential for users to become emotionally attached to AI tools.",
    "category": "technology",
    "bias": "left"
  },
  {
    "url": "https://www.theverge.com/news/718191/google-apple-intelligence-dunk-pixel-10-ad",
    "title": "Google dunks on Apple Intelligence in new Pixel 10 ad",
    "text": "Posts from this author will be added to your daily email digest and your homepage feed.\n\nApple sold its iPhone 16 devices last year with a promise that a new AI-powered version of Siri would soon be a lot more personalized thanks to Apple Intelligence. Almost a year later, that Siri upgrade still isn\u2019t here, and Apple was forced to delay its promised improvements and remove an iPhone 16 commercial instead. Now, Google doesn\u2019t want anyone to forget about this Apple Intelligence debacle.\n\nIn a new Pixel 10 ad, Google dunks on Apple\u2019s failed promise of Siri AI improvements, with a narrator that suggests you could \u201cjust change your phone\u201d if you bought \u201ca new phone because of a feature that\u2019s coming soon, but it\u2019s been coming soon for a full year.\u201d\n\nThe 30-second spot appeared on YouTube and X today, teasing the launch of Google\u2019s new Pixel 10 devices on August 20th. Not that there\u2019s much left to tease, thanks to Google\u2019s own mishaps, an official teaser image, and plenty of other leaks.\n\nGoogle\u2019s latest ad comes just a day after a report from Bloomberg\u2019s Mark Gurman shed some additional light on Apple\u2019s AI delays. In a recent all-hands meeting, Apple\u2019s SVP of software Craig Federighi reportedly put the delay down to Apple\u2019s issues of trying to use a hybrid architecture for Siri. Apple is now reportedly working on a new version of Siri with an updated architecture.\n\n\u201cThis has put us in a position to not just deliver what we announced, but to deliver a much bigger upgrade than we envisioned,\u201d said Federighi. \u201cThere is no project people are taking more seriously.\u201d Federighi previously revealed in June that it was \u201cgoing to take us longer than we thought\u201d to deliver the promised Siri upgrade.",
    "summary": "Apple sold its iPhone 16 devices last year with a promise that a new AI-powered version of Siri would soon be a lot more personalized thanks to Apple Intelligence.\nAlmost a year later, that Siri upgrade still isn\u2019t here, and Apple was forced to delay its promised improvements and remove an iPhone 16 commercial instead.\nNow, Google doesn\u2019t want anyone to forget about this Apple Intelligence debacle.\nGoogle\u2019s latest ad comes just a day after a report from Bloomberg\u2019s Mark Gurman shed some additional light on Apple\u2019s AI delays.\nApple is now reportedly working on a new version of Siri with an updated architecture.",
    "category": "technology",
    "bias": "left"
  },
  {
    "url": "https://www.wired.com/story/wired-ai-power-summit/",
    "title": "Join Us for WIRED\u2019s AI Power Summit",
    "text": "The strength and capabilities of generative AI are accelerating at a dizzying pace. If you\u2019re finding it difficult to keep up, we get it. That\u2019s why WIRED is hosting its first AI Power Summit on September 15 in New York City.\n\nWe\u2019ve curated a series of panels and conversations with experts who will distill and discuss the implications of today\u2019s most crucial AI-related news. We\u2019ll break down the White House\u2019s AI Action Plan and examine its consequences across industries; explore how emerging regulations could redefine the trajectory of innovation and shape public policy; and discuss who stands to gain\u2014and who stands to lose\u2014in AI\u2019s next chapter.\n\nExpect to hear from some of your favorite WIRED writers and editors, and leaders across technology, politics, and media. More details will be announced in the coming weeks.\n\nWIRED subscribers will have exclusive first access to a livestream of the event. Not yet a WIRED subscriber? Join today!\n\nIf you feel overwhelmed by the influx of news about the rapidly evolving technological breakthrough, or if you\u2019re hungry for an in-depth discussion on the topic led by experts you can trust, you certainly won\u2019t want to miss this. We hope to see you there.",
    "summary": "The strength and capabilities of generative AI are accelerating at a dizzying pace.\nIf you\u2019re finding it difficult to keep up, we get it.\nThat\u2019s why WIRED is hosting its first AI Power Summit on September 15 in New York City.\nWe\u2019ve curated a series of panels and conversations with experts who will distill and discuss the implications of today\u2019s most crucial AI-related news.\nExpect to hear from some of your favorite WIRED writers and editors, and leaders across technology, politics, and media.",
    "category": "technology",
    "bias": "lean right"
  },
  {
    "url": "https://www.wired.com/story/missing-hiker-ai-drone-recovery/",
    "title": "A Hiker Was Missing for Nearly a Year\u2014Until an AI System Recognized His Helmet",
    "text": "How long does it take to identify the helmet of a hiker lost in a 183-hectare mountain area, analyzing 2,600 frames taken by a drone from approximately 50 meters away? If done with a human eye, weeks or months. If analyzed by an artificial intelligence system, one afternoon. The National Alpine and Speleological Rescue Corps, known by it\u2019s Italian initialism CNSAS, relied on AI to find the body of a person missing in Italy's Piedmont region on the north face of Monviso\u2014the highest peak in the Cottian Alps\u2014since September 2024.\n\nAccording to Saverio Isola, the CNSAS drone pilot who intervened along with his colleague Giorgio Viana, the operation\u2014including searching for any sign of the missing hiker, the discovery and recovery of his body, and a stoppage due to bad weather\u2014lasted less than three days.\n\nThe Recovery Operations\n\nWith his back to the ground, his gaze fixed on the mountains, 600 meters below the summit, the body of 64-year-old Ligurian doctor Nicola Ivaldo was found on the morning of Thursday, July 31, more than 10 months after his disappearance, thanks to his helmet that clashed with the rest of the landscape.\n\n\"It was the AI software that identified some pixels of a different color in the images taken on Tuesday,\" explains Isola, reconstructing step-by-step the operation that led to the discovery and recovery of the remains located at an altitude of approximately 3,150 meters, in the rightmost of the three ravines that cut through the north face of Monviso, above a hanging glacier.\n\nThe team collected all the images in five hours with just two drones on the morning of Tuesday, July 29, and analyzed them using AI software during the afternoon of the same day. By that evening, the rescuers already had a series of \"suspicious spots\" to check. Only fog and bad weather the following day delayed the operations.\n\n\"We woke up at 4 am to reach a very distant point with good visibility on the channel where the red pixels had been detected, and we used the drone to see if it was indeed the helmet,\" says Isola. \"Then we took all the necessary photos and measurements, sending the information to the rescue coordination center, which was then able to dispatch the Fire Brigade helicopter for the recovery and police operations.\"\n\nThe Role of AI\n\nEvery drone operation is part of a rigorous method developed by CNSAS in coordination with ENAC, the national agency that oversees civil aviation. \"We've been using drones for about five years, and for about a year and a half we've been integrating color and shape recognition technologies, developing them month by month,\" Isola explains. \"But all of this would be useless without the teams of technicians.\"\n\nInformation from Ivaldo's cell phone was immediately invaluable. The two drone pilots who navigated the area were aided by the experience and knowledge of four expert mountain rescuers. \"It's a human achievement, but without technology, it would have been an impossible mission. It's a team success,\" said Isola.",
    "summary": "How long does it take to identify the helmet of a hiker lost in a 183-hectare mountain area, analyzing 2,600 frames taken by a drone from approximately 50 meters away?\nIf analyzed by an artificial intelligence system, one afternoon.\nThe team collected all the images in five hours with just two drones on the morning of Tuesday, July 29, and analyzed them using AI software during the afternoon of the same day.\nThe two drone pilots who navigated the area were aided by the experience and knowledge of four expert mountain rescuers.\n\"It's a human achievement, but without technology, it would have been an impossible mission.",
    "category": "technology",
    "bias": "left"
  },
  {
    "url": "https://gizmodo.com/ai-wont-boost-human-productivity-just-yet-a-new-paper-from-the-federal-reserve-says-2000637521",
    "title": "AI Won\u2019t Boost Human Productivity Yet, U.S. Federal Reserve Says",
    "text": "Generative AI is not just another tech hype cycle that is bound to die down but is instead a game-changer for human productivity, according to the Federal Reserve. The big caveat, though, is the road to get there will be \u201cinherently slow\u201d and \u201cfraught with risk.\u201d\n\nIn a recent paper published by the Fed Board of Governors, researchers suggest that the hype around generative AI is probably not a bubble in the long run and that the technology will be a serious macroeconomic force, proving to have revolutionary effects for labor productivity akin to electricity and the microscope.\n\nThe idea that generative AI will make the workforce more productive isn\u2019t a groundbreaking one. It\u2019s been lauded by corporate executives and many AI bulls alike since OpenAI\u2019s generative AI model ChatGPT sparked the AI craze.\n\nBut what\u2019s significant is that the country\u2019s most powerful economic institution has just voiced notable confidence in the technology\u2019s potential. Albeit with a catch.\n\nAI could be the next microscope\n\nThe paper divides technological innovations into three categories. First, you have innovations like the light bulb, which dramatically increased productivity initially by allowing workers to not be limited to daylight. But once the technology was adopted widely, the lightbulb stopped providing additional value to workplace productivity.\n\n\u201cIn contrast, two types of technologies stand out as having longer-lived effects on productivity growth,\u201d the researchers write, and AI has characteristics of both.\n\nThe first are \u201cgeneral-purpose technologies,\u201d like the electric dynamo or the computer. The electric dynamo was the first practical electric generator, and it continued to deliver accelerating productivity growth even after widespread adoption because it spurred related innovations and continued to improve on itself.\n\nThe researchers say that generative AI is already showing signs that it fits the bill. You have specialized LLMs for specific domains like OpenAI\u2019s LegalGPT meant to assist in legal matters, and \u201ccopilots\u201d like Microsoft\u2019s Copilot product, which is meant to increase office productivity by integrating generative AI into corporate workstreams. Fed researchers think even more knock-on innovations are to come, and that wave will be led by digital native companies.\n\nAnd it\u2019s evident that the core technology is rapidly innovating and will likely continue to do so as companies develop the technology with an aim to achieve artificial general intelligence. In the meantime, the paper points out, the technology\u2019s rapid growth has already given us further innovations like agentic AI and landmark AI models like Deepseek\u2019s R1.\n\nThe second type of technology is called \u201cinventions of methods of invention,\u201d the most prominent examples being the microscope or the printing press. Although a microscope has now become a common tool, it continues to raise levels of human productivity by enabling research and development projects.\n\nGenerative AI has been helpful in simulations to understand the nature of the universe, in novel drug discoveries, and more. And the paper notes that there has been a huge spike, starting in 2023, of companies citing AI within research and development contexts and in corporate earnings calls, showing that perhaps AI\u2019s integration with corporate innovation has already begun.\n\nThere\u2019s always a catch\n\nAlas, this confidence comes with a caveat. AI will be a boon for economic and productivity growth, but it is unlikely to happen overnight.\n\nThe Fed\u2019s paper says the biggest challenge with generative AI right now isn\u2019t the tech itself: it\u2019s getting people and businesses to actually use it. While researchers are starting to adopt it more, most companies outside of tech and the scientific fields haven\u2019t worked it into their daily operations yet, with the exception of the finance industry. And industry surveys show that AI adoption is far higher within large firms than small ones.\n\nSo while generative AI is likely to boost how productive we are overall, the impact will be slow. That\u2019s because it takes time, money, and other supporting tech like user interfaces, robotics, and AI agents to make AI really useful across the economy. The authors compare it to past big tech changes, like advances in computation, which accumulated for decades before causing a productivity boom.\n\nThe timeline for that boom is still unknown. Goldman Sachs economists think AI\u2019s effects on labor productivity and GDP growth in the U.S. will start to show in 2027 and will accelerate to a peak in the 2030s.\n\nAnother risk the Fed points out comes with building infrastructure for anticipated demand. A widespread adoption of generative AI means significant need for investment in data centers and electricity generation. But investing too quickly can have \u201cdisastrous consequences\u201d when demand doesn\u2019t grow as expected, the Fed warns, similar to how railroad overexpansion in the 1800s led to an economic depression towards the end of the century.\n\nDespite the caveats, the Fed is confident that generative AI will be transformative for productivity. But whether that transformation continues to accelerate perpetually and have as big of an effect as the electric dynamo or the microscope will depend on the extent and speed of the technology\u2019s adoption.",
    "summary": "Generative AI is not just another tech hype cycle that is bound to die down but is instead a game-changer for human productivity, according to the Federal Reserve.\nIt\u2019s been lauded by corporate executives and many AI bulls alike since OpenAI\u2019s generative AI model ChatGPT sparked the AI craze.\nGenerative AI has been helpful in simulations to understand the nature of the universe, in novel drug discoveries, and more.\nSo while generative AI is likely to boost how productive we are overall, the impact will be slow.\nA widespread adoption of generative AI means significant need for investment in data centers and electricity generation.",
    "category": "technology",
    "bias": "lean right"
  },
  {
    "url": "https://www.wired.com/story/security-news-this-week-google-will-use-ai-to-guess-peoples-ages-based-on-search-history/",
    "title": "Google Will Use AI to Guess People\u2019s Ages Based on Search History",
    "text": "Last week, the United Kingdom began requiring residents to verify their ages before accessing online pornography and other adult content, all in the name of protecting children. Almost immediately, things did not go as planned\u2014although, they did go as expected.\n\nAs experts predicted, UK residents began downloading virtual private networks (VPNs) en masse, allowing them to circumvent age verification, which can require users to upload their government IDs, by making it look like they\u2019re in a different country. The UK\u2019s Online Safety Act is just one part of a wave of age-verification efforts around the world. And while these laws may keep some kids from accessing adult content, some experts warn that they also create security and privacy risks for everyone.\n\nRussia\u2019s state-backed hacking group Turla is known for its bold, creative attacks, such as masking their communications via satellite or piggybacking on other hackers\u2019 attacks to avoid detection. The group, which is part of the Russian FSB intelligence agency, is now using its access to the country\u2019s internet providers to trick foreign officials into downloading spyware that breaks encryption, allowing Turla\u2019s hackers to access their private information.\n\nAnd that\u2019s not all. Each week, we round up the security and privacy news we didn\u2019t cover in depth ourselves. Click the headlines to read the full stories. And stay safe out there.\n\nGoogle is rolling out an AI-powered age-estimation system to apply content protections to Search and YouTube, even for users who haven\u2019t provided their age. The system is launching in the EU, where digital safety regulations mandate that platforms take steps to protect minors from potentially harmful content.\n\nInstead of relying solely on user-input data, Google says it will infer age using a \u201cvariety of signals\u201d and other metadata to determine if a user should be shown restricted results. Privacy advocates say the move risks inaccuracies and raises questions about transparency and consent.\n\nGoogle claims the changes align with regulatory expectations and will help protect younger users from inappropriate content. Still, the idea that platforms can algorithmically infer personal traits like age\u2014and restrict content based solely on those assumptions\u2014adds a new wrinkle to long-standing debates over moderation, censorship, and digital privacy.\n\nJust 24 hours after naming Jen Easterly as West Point\u2019s Distinguished Chair in Social Sciences, the Army rescinded the appointment following far-right criticism. The former Cybersecurity and Infrastructure Security Agency (CISA) director and academy alum had been lauded for her decades of service. But backlash erupted online after activist Laura Loomer claimed Easterly had ties to the Biden-era Disinformation Governance Board.\n\nNina Jankowicz, who served as executive director of the board, denied having worked with Easterly in a post on BlueSky, calling the episode yet another example of how we\u2019re all living in \u201cthe stupidest timeline.\u201d\n\nNevertheless, Army secretary Dan Driscoll canceled Easterly\u2019s contract and ordered a full review of West Point\u2019s hiring policies. The Army also suspended the practice of allowing outside groups to help select faculty. The reversal marks the second high-profile clash involving former CISA leaders and political pressure following Donald Trump\u2019s revocation of Chris Krebs\u2019 security clearance earlier this year.\n\nA bipartisan bill from US senators Amy Klobuchar and Ted Cruz could let lawmakers demand the removal of online posts showing their home addresses or travel plans, Rolling Stone reports. The proposal, which could pass by unanimous consent, is framed as a response to growing threats against public officials\u2014especially after the assassination of Minnesota legislator Melissa Hortman last month.\n\nWatchdogs joined dozens of media outlets in warning that the bill could chill reporting and enable selective censorship. While the legislation includes a nominal exemption for journalists, critics say it remains vague enough to allow members of Congress to sue outlets or demand takedowns of legitimate news stories.",
    "summary": "The UK\u2019s Online Safety Act is just one part of a wave of age-verification efforts around the world.\nAnd while these laws may keep some kids from accessing adult content, some experts warn that they also create security and privacy risks for everyone.\nEach week, we round up the security and privacy news we didn\u2019t cover in depth ourselves.\nGoogle is rolling out an AI-powered age-estimation system to apply content protections to Search and YouTube, even for users who haven\u2019t provided their age.\nGoogle claims the changes align with regulatory expectations and will help protect younger users from inappropriate content.",
    "category": "technology",
    "bias": "left"
  },
  {
    "url": "https://gizmodo.com/disney-ai-moana-tron-ares-backlash-lawsuits-2000638481",
    "title": "Report: Disney\u2019s Attempts to Experiment With Generative AI Have Already Hit Major Hurdles",
    "text": "As Silicon Valley has pushed the world more and more into trying to make the generative AI boom sustain itself, Hollywood is still standing on the precipice of a transformative moment. Studios are grappling with the purported potential (and demands for cost savings) artificial intelligence models may bring, weighed against the legal minefields exploiting such technologies can represent\u2014and an increasing public backlash to the technology.\n\nDisney is certainly no exception, as the company is already familiar with both the legal headaches and the PR nightmare generative AI can represent. But a new report from the Wall Street Journal claims that there\u2019ve been even more attempts behind the scenes at Disney\u2019s studio to try and utilize generative AI technologies\u2026 neither of which purportedly went very far or well, for very different reasons.\n\nTwo upcoming productions that tried to navigate potential use of generative AI mentioned in the WSJ report are the upcoming live-action Moana remake and Tron: Ares. For the former, Disney reportedly planned to work with an AI company called Metaphysic to create a digital deepfake of actor Dwayne Johnson, set to reprise his role as the demigod Maui in the remake. In an attempt to reduce the number of days Johnson would be required on set for production, the alleged plan was to have Johnson\u2019s cousin, Tanoai Reed, act as a stand-in who would have Johnson\u2019s deepfaked face put over his performance in post-production.\n\nAlthough the plan was for a \u201csmall number of shots,\u201d according to WSJ\u2019s report, after 18 months of negotiation and work between Disney and Metaphysic, none of the shots using Reed\u2019s performance will be in the final movie when it releases in July 2026. WSJ\u2019s report cited concerns over data security on Disney\u2019s end, as well as the legal question that lingers over any broader embrace of generative AI technology in Hollywood: who, exactly, owns the end product when generative AI models are used to create even a part of it?\n\nThat thorny question of ownership has already seen Disney take legal action against AI companies over claims of illegal misuse of copyrighted material to train their models. In June this year, Disney teamed up with Universal to sue Midjourney over what the suit described as a \u201cbottomless pit of plagiarism,\u201d accusing the AI company\u2019s image generator of breaching copyright laws to distribute and create images trained on the studios\u2019 library of characters and franchises.\n\nBut copyright is not the only concern Disney faces when it comes to ideas around generative AI: the studio is also increasingly navigating potential publicity nightmares as social backlash to the use of the technology increases.\n\nIn another example in WSJ\u2019s report, it\u2019s alleged that Disney executives pitched creatives on the set of Tron: Ares on including a generative AI character in the film, which itself is already about artificial intelligences escaping the digital world of \u201cThe Grid\u201d to be exploited as military contractors in the real world. According to WSJ, the character would\u2019ve been called \u201cBit\u201d and acted as a potential companion to Jeff Bridges\u2019 returning Kevin Flynn, and built off of context provided by a writer, the generated character would then be recorded and deliver lines performed by an actor, responding as if the model itself were Bit.\n\nThe report claims that similarly the idea was stymied again by legal discussions at the time, amid negotiations with unions, as well as the fact that Disney executives were purportedly told to drop the idea internally because \u201cthe company couldn\u2019t risk the bad publicity.\u201d\n\nDisney is, of course, no stranger to public embarrassment when it comes to its properties and AI, either. Marvel was lambasted for the use of generative AI to create the opening title sequence to its Disney+ series Secret Invasion in 2023, and found itself defending itself from accusations of its use once more for the early marketing campaign for Fantastic Four: First Steps. Earlier this summer, Disney\u2019s investment into Epic Games was touted through the arrival of a generative-AI-enhanced Darth Vader avatar in Fortnite to promote the battle royale video game\u2019s then-ongoing Star Wars event, \u201cGalactic Battle\u201d, where Darth Vader could be recruited by players, using a deepfake model of the late James Earl Jones\u2019 voice to interact with players in real time.\n\nPlayers promptly figured out ways to get around Epic\u2019s content restrictions and get the generative Vader to swear and use slurs. Although Epic managed to fix the bugs within 30 minutes of the Vader character\u2019s appearance going live in Fortnite, several videos of the exploits went viral on social media. SAG-AFTRA also filed an unfair labor practice charge against Epic over the use of generative AI denying a human actor the chance to voice the role (Jones\u2019 estate had already sold the rights to his voice to the Ukrainian tech company Respeecher in 2022 before his passing), but rumors recently swirled that the union dropped the charges in the wake of signing a new contract last month.\n\nThe legal outcome of Disney and Universal\u2019s lawsuit is still to be decided, but what is seemingly clear is that the potential AI takeover of Hollywood that has been feared with the proliferation of generative AI may not be as close as some people (and some companies) expect.",
    "summary": "As Silicon Valley has pushed the world more and more into trying to make the generative AI boom sustain itself, Hollywood is still standing on the precipice of a transformative moment.\nDisney is certainly no exception, as the company is already familiar with both the legal headaches and the PR nightmare generative AI can represent.\nBut a new report from the Wall Street Journal claims that there\u2019ve been even more attempts behind the scenes at Disney\u2019s studio to try and utilize generative AI technologies\u2026 neither of which purportedly went very far or well, for very different reasons.\nTwo upcoming productions that tried to navigate potential use of generative AI mentioned in the WSJ report are the upcoming live-action Moana remake and Tron: Ares.\nPlayers promptly figured out ways to get around Epic\u2019s content restrictions and get the generative Vader to swear and use slurs.",
    "category": "technology",
    "bias": "left"
  },
  {
    "url": "https://gizmodo.com/read-this-before-you-trust-any-ai-written-code-2000637243",
    "title": "Read This Before You Trust Any AI-Written Code",
    "text": "We are in the era of vibe coding, allowing artificial intelligence models to generate code based on a developer\u2019s prompt. Unfortunately, under the hood, the vibes are bad. According to a recent report published by data security firm Veracode, about half of all AI-generated code contains security flaws.\n\nVeracode tasked over 100 different large language models with completing 80 separate coding tasks, from using different coding languages to building different types of applications. Per the report, each task had known potential vulnerabilities, meaning the models could potentially complete each challenge in a secure or insecure way. The results were not exactly inspiring if security is your top priority, with just 55% of tasks completed ultimately generating \u201csecure\u201d code.\n\nNow, it\u2019d be one thing if those vulnerabilities were little flaws that could easily be patched or mitigated. But they\u2019re often pretty major holes. The 45% of code that failed the security check produced a vulnerability that was part of the Open Worldwide Application Security Project\u2019s top 10 security vulnerabilities\u2014issues like broken access control, cryptographic failures, and data integrity failures. Basically, the output has big enough issues that you wouldn\u2019t want to just spin it up and push it live, unless you\u2019re looking to get hacked.\n\nPerhaps the most interesting finding of the study, though, is not simply that AI models are regularly producing insecure code. It\u2019s that the models don\u2019t seem to be getting any better. While syntax has significantly improved over the last two years, with LLMs producing compilable code nearly all the time now, the security of said code has basically remained flat the whole time. Even newer and larger models are failing to generate significantly more secure code.\n\nThe fact that the baseline of secure output for AI-generated code isn\u2019t improving is a problem, because the use of AI in programming is getting more popular, and the surface area for attack is increasing. Earlier this month, 404 Media reported on how a hacker managed to get Amazon\u2019s AI coding agent to delete the files of computers that it was used on by injecting malicious code with hidden instructions into the GitHub repository for the tool.\n\nMeanwhile, as AI agents become more common, so do agents capable of cracking the very same code. Recent research out of the University of California, Berkeley, found that AI models are getting very good at identifying exploitable bugs in code. So AI models are consistently generating insecure code, and other AI models are getting really good at spotting those vulnerabilities and exploiting them. That\u2019s all probably fine.",
    "summary": "According to a recent report published by data security firm Veracode, about half of all AI-generated code contains security flaws.\nThe results were not exactly inspiring if security is your top priority, with just 55% of tasks completed ultimately generating \u201csecure\u201d code.\nPerhaps the most interesting finding of the study, though, is not simply that AI models are regularly producing insecure code.\nRecent research out of the University of California, Berkeley, found that AI models are getting very good at identifying exploitable bugs in code.\nSo AI models are consistently generating insecure code, and other AI models are getting really good at spotting those vulnerabilities and exploiting them.",
    "category": "technology",
    "bias": "right"
  },
  {
    "url": "https://gizmodo.com/apple-ceo-tim-cook-calls-ai-bigger-than-the-internet-in-rare-all-hands-meeting-2000638858",
    "title": "Apple CEO Tim Cook Calls AI \u2018Bigger Than the Internet\u2019 in Rare All-Hands Meeting",
    "text": "In a global all-hands meeting hosted from Apple\u2019s headquarters in Cupertino, California, CEO Tim Cook seemed to admit to what analysts and Apple enthusiasts around the world had been raising concerns about: that Apple has fallen behind competitors in the AI race. And Cook promised employees that the company will be doing everything to catch up.\n\n\u201cApple must do this. Apple will do this. This is sort of ours to grab,\u201d Cook said, according to Bloomberg, and called the AI revolution \u201cas big or bigger\u201d than the internet.\n\nThe meeting took place a day after Apple reported better than expected revenue in its quarterly earnings report, and that sent the company\u2019s stock soaring. The report came in a week already marked by great tech earnings results, partially driven by AI. But unlike Meta and Microsoft, Apple\u2019s rise in revenue was attributable to iPhone sales and not necessarily a strength in AI.\n\nIn the earnings call following the report, Cook told investors that Apple was planning to \u201csignificantly\u201d increase its investments in AI and was open to acquisitions to do so. He also said that the company is actively \u201creallocating a fair number of people to focus on AI features.\u201d\n\nCook echoed those sentiments in Friday\u2019s meeting, saying that the company will be making the necessary investments in AI to catch up to the moment.\n\nThe AI-enhanced Siri is back and allegedly better than ever\n\nApple has been working on integrating advanced AI into its product lineup for the past year or so under its Apple Intelligence initiative, which the company unveiled at the June 2024 Worldwide Developers Conference. The move was met by celebration and criticism even then: Apple\u2019s big bet on AI was coming a good year or so after competitors like OpenAI, Google, Microsoft, and Meta scaled up their offerings.\n\nEven so, the company\u2019s progress on Apple Intelligence has been slow. Apple was supposed to unveil an AI-enhanced Siri earlier this year, and even released ads for the new iPhone with AI-enhanced Siri capabilities, but the Cupertino giant pushed that reveal back at the last minute, reportedly to next spring, though nothing is officially confirmed.\n\nThe switch-up caused major backlash from investors and customers, two major lawsuits, and a complete corporate overhaul.\n\nCook said on Friday that 12,000 workers were hired in the last year, with 40% of them joining research and development teams.\n\nThe leadership overhaul following the fallout of LLM Siri has \u201csupercharged\u201d the company\u2019s work in AI development, senior vice president of software engineering Craig Federighi said at the meeting.\n\nAccording to Federighi, the main problem with the LLM Siri rollout was that Apple tried to build a \u201chybrid architecture\u201d that utilized two different software systems. That plan has now been scratched, and Federighi seemed confident in LLM Siri\u2019s future this time around, claiming that the new \u201cend-to-end revamp of Siri\u201d will now be delivering \u201ca much bigger upgrade than we envisioned.\u201d\n\nChips are center stage\n\nAlso key to the new AI strategy, according to Cook, is chip development.\n\nApple has been working on designing in-house AI chips for some time now, according to a Wall Street Journal report from last year, in a project internally code-named ACDC (standing for Apple Chips in Data Center). The tech giant has reportedly teamed up with Broadcom to develop its first AI chip code-named Baltra, according to a report last year in The Information, and Apple is expecting to begin mass production by 2026.\n\nApple\u2019s lagging could be on-brand\n\nDespite being a global leader in tech and a household name in consumer electronics, Apple is nowhere near the top when it comes to the AI race.\n\nBut while that scares some Apple fans and investors, others think it\u2019s actually kind of on-brand. Tim Cook indicated Friday that he belongs to the latter camp.\n\n\u201cWe\u2019ve rarely been first,\u201d Cook said at the meeting. \u201cThere was a PC before the Mac; there was a smartphone before the iPhone; there were many tablets before the iPad; there was an MP3 player before iPod.\u201d\n\nCook has a point. Apple isn\u2019t necessarily known for spearheading new technology, but the company\u2019s strength comes from perfecting said technology and making products that become highly dominant in their respective markets. And if Apple makes the right moves in developing and scaling its AI product offerings, Cook could potentially add AI to that list as well.",
    "summary": "This is sort of ours to grab,\u201d Cook said, according to Bloomberg, and called the AI revolution \u201cas big or bigger\u201d than the internet.\nThe meeting took place a day after Apple reported better than expected revenue in its quarterly earnings report, and that sent the company\u2019s stock soaring.\nThe report came in a week already marked by great tech earnings results, partially driven by AI.\nIn the earnings call following the report, Cook told investors that Apple was planning to \u201csignificantly\u201d increase its investments in AI and was open to acquisitions to do so.\nAnd if Apple makes the right moves in developing and scaling its AI product offerings, Cook could potentially add AI to that list as well.",
    "category": "technology",
    "bias": "left"
  },
  {
    "url": "https://tech.yahoo.com/ai/articles/google-commits-1-billion-ai-160316025.html",
    "title": "Google commits $1 billion for AI training at US universities",
    "text": "By Kenrick Cai\n\nSAN FRANCISCO (Reuters) -Alphabet's Google on Wednesday announced a three-year, $1 billion commitment to provide artificial intelligence training and tools to U.S. higher education institutions and nonprofits.\n\nMore than 100 universities have signed on to the initiative so far, including some of the nation's largest public university systems such as Texas A&M and the University of North Carolina.\n\nAdvertisement Advertisement\n\nAdvertisement\n\nParticipating schools may receive cash funding and resources, such as cloud computing credits towards AI training for students as well as research on AI-related topics.\n\nThe billion-dollar figure also includes the value of paid AI tools, such as an advanced version of the Gemini chatbot, which Google will give to college students for free.\n\nGoogle hopes to expand the program to every accredited nonprofit college in the U.S. and is discussing similar plans in other countries, Senior Vice President James Manyika said in an interview.\n\nHe declined to specify how much Google is earmarking in direct funds to external institutions relative to footing its own cloud and subscription bills.\n\nAdvertisement Advertisement\n\nAdvertisement\n\nThe announcement comes as rivals like OpenAI, Anthropic and Amazon have made similar pushes around AI in education as the technology pervades society. Microsoft in July pledged $4 billion to bolster AI in education globally.\n\nBy evangelizing their products to students, tech firms further stand to win business deals once those users enter the workforce.\n\nA growing body of research has mapped concerns around AI's role in education, from enabling cheating to eroding critical thinking, prompting some schools to consider bans.\n\nManyika said Google had not faced resistance from administrators since it began to plot its education initiative earlier this year, but \"many more questions\" about AI-related concerns remain.\n\nAdvertisement Advertisement\n\nAdvertisement\n\n\"We're hoping to learn together with these institutions about how best to use these tools,\" he said, adding that the insights could help shape future product decisions.\n\n(Reporting by Kenrick Cai; Editing by Jamie Freed)",
    "summary": "By Kenrick CaiSAN FRANCISCO (Reuters) -Alphabet's Google on Wednesday announced a three-year, $1 billion commitment to provide artificial intelligence training and tools to U.S. higher education institutions and nonprofits.\nAdvertisement AdvertisementAdvertisementParticipating schools may receive cash funding and resources, such as cloud computing credits towards AI training for students as well as research on AI-related topics.\nThe billion-dollar figure also includes the value of paid AI tools, such as an advanced version of the Gemini chatbot, which Google will give to college students for free.\nAdvertisement AdvertisementAdvertisementThe announcement comes as rivals like OpenAI, Anthropic and Amazon have made similar pushes around AI in education as the technology pervades society.\nMicrosoft in July pledged $4 billion to bolster AI in education globally.",
    "category": "technology",
    "bias": "left"
  }
]